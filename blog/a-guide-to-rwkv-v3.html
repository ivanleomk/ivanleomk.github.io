<!--
  Copyright (c) 2016-2024 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="A guide to a strong open-source transformer alternative"><link rel=canonical href=https://ivanleo.com/blog/a-guide-to-rwkv-v3.html><link rel=prev href=reinventing-gandalf.html><link rel=next href=gpt-react.html><link rel=alternate type=application/rss+xml title="RSS feed" href=../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../feed_rss_updated.xml><link rel=icon href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.44"><title>A guide to RWKV V3 - Ivan's Blog</title><link rel=stylesheet href=../assets/stylesheets/main.0253249f.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../assets/_mkdocstrings.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-MM8QMY5JWN"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-MM8QMY5JWN",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-MM8QMY5JWN",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta property=og:type content=website><meta property=og:title content="A guide to RWKV V3 - Ivan's Blog"><meta property=og:description content="A guide to a strong open-source transformer alternative"><meta property=og:image content=https://ivanleo.com/assets/images/social/blog/posts/a-guide-to-rkwv.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta property=og:url content=https://ivanleo.com/blog/a-guide-to-rwkv-v3.html><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="A guide to RWKV V3 - Ivan's Blog"><meta name=twitter:description content="A guide to a strong open-source transformer alternative"><meta name=twitter:image content=https://ivanleo.com/assets/images/social/blog/posts/a-guide-to-rkwv.png></head> <body dir=ltr> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#introduction class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../index.html title="Ivan's Blog" class="md-header__button md-logo" aria-label="Ivan's Blog" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Ivan's Blog </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> A guide to RWKV V3 </span> </div> </div> </div> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> About me </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=index.html class=md-tabs__link> Blog </a> </li> <li class=md-tabs__item> <a href=../work.html class=md-tabs__link> Work with me </a> </li> <li class=md-tabs__item> <a href=../newsletter.html class=md-tabs__link> Newsletter </a> </li> <li class=md-tabs__item> <a href=../talk.html class=md-tabs__link> Talks </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../index.html title="Ivan's Blog" class="md-nav__button md-logo" aria-label="Ivan's Blog" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> Ivan's Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../index.html class=md-nav__link> <span class=md-ellipsis> About me </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <a href=index.html class=md-nav__link> <span class=md-ellipsis> Index </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex=0> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=archive/2025.html class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> <li class=md-nav__item> <a href=archive/2024.html class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> <li class=md-nav__item> <a href=archive/2023.html class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex=0> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=category/ai.html class=md-nav__link> <span class=md-ellipsis> AI </span> </a> </li> <li class=md-nav__item> <a href=category/ai-engineering.html class=md-nav__link> <span class=md-ellipsis> AI Engineering </span> </a> </li> <li class=md-nav__item> <a href=category/advice.html class=md-nav__link> <span class=md-ellipsis> Advice </span> </a> </li> <li class=md-nav__item> <a href=category/applied-ai.html class=md-nav__link> <span class=md-ellipsis> Applied AI </span> </a> </li> <li class=md-nav__item> <a href=category/braintrust.html class=md-nav__link> <span class=md-ellipsis> Braintrust </span> </a> </li> <li class=md-nav__item> <a href=category/career.html class=md-nav__link> <span class=md-ellipsis> Career </span> </a> </li> <li class=md-nav__item> <a href=category/clustering.html class=md-nav__link> <span class=md-ellipsis> Clustering </span> </a> </li> <li class=md-nav__item> <a href=category/comfyui.html class=md-nav__link> <span class=md-ellipsis> ComfyUI </span> </a> </li> <li class=md-nav__item> <a href=category/debugging.html class=md-nav__link> <span class=md-ellipsis> Debugging </span> </a> </li> <li class=md-nav__item> <a href=category/deployment.html class=md-nav__link> <span class=md-ellipsis> Deployment </span> </a> </li> <li class=md-nav__item> <a href=category/diffusion-models.html class=md-nav__link> <span class=md-ellipsis> Diffusion Models </span> </a> </li> <li class=md-nav__item> <a href=category/documentation.html class=md-nav__link> <span class=md-ellipsis> Documentation </span> </a> </li> <li class=md-nav__item> <a href=category/evals.html class=md-nav__link> <span class=md-ellipsis> Evals </span> </a> </li> <li class=md-nav__item> <a href=category/evaluation.html class=md-nav__link> <span class=md-ellipsis> Evaluation </span> </a> </li> <li class=md-nav__item> <a href=category/evaluations.html class=md-nav__link> <span class=md-ellipsis> Evaluations </span> </a> </li> <li class=md-nav__item> <a href=category/instructor.html class=md-nav__link> <span class=md-ellipsis> Instructor </span> </a> </li> <li class=md-nav__item> <a href=category/llm.html class=md-nav__link> <span class=md-ellipsis> LLM </span> </a> </li> <li class=md-nav__item> <a href=category/llms.html class=md-nav__link> <span class=md-ellipsis> LLMs </span> </a> </li> <li class=md-nav__item> <a href=category/mcps.html class=md-nav__link> <span class=md-ellipsis> MCPs </span> </a> </li> <li class=md-nav__item> <a href=category/mac.html class=md-nav__link> <span class=md-ellipsis> Mac </span> </a> </li> <li class=md-nav__item> <a href=category/machine-learning.html class=md-nav__link> <span class=md-ellipsis> Machine Learning </span> </a> </li> <li class=md-nav__item> <a href=category/modal.html class=md-nav__link> <span class=md-ellipsis> Modal </span> </a> </li> <li class=md-nav__item> <a href=category/personal.html class=md-nav__link> <span class=md-ellipsis> Personal </span> </a> </li> <li class=md-nav__item> <a href=category/personal-development.html class=md-nav__link> <span class=md-ellipsis> Personal Development </span> </a> </li> <li class=md-nav__item> <a href=category/python.html class=md-nav__link> <span class=md-ellipsis> Python </span> </a> </li> <li class=md-nav__item> <a href=category/rag.html class=md-nav__link> <span class=md-ellipsis> RAG </span> </a> </li> <li class=md-nav__item> <a href=category/rwkv.html class=md-nav__link> <span class=md-ellipsis> RWKV </span> </a> </li> <li class=md-nav__item> <a href=category/synthetic-data.html class=md-nav__link> <span class=md-ellipsis> Synthetic Data </span> </a> </li> <li class=md-nav__item> <a href=category/testing.html class=md-nav__link> <span class=md-ellipsis> Testing </span> </a> </li> <li class=md-nav__item> <a href=category/trends.html class=md-nav__link> <span class=md-ellipsis> Trends </span> </a> </li> <li class=md-nav__item> <a href=category/ui-generation.html class=md-nav__link> <span class=md-ellipsis> UI Generation </span> </a> </li> <li class=md-nav__item> <a href=category/uiux.html class=md-nav__link> <span class=md-ellipsis> UI/UX </span> </a> </li> <li class=md-nav__item> <a href=category/voice.html class=md-nav__link> <span class=md-ellipsis> Voice </span> </a> </li> <li class=md-nav__item> <a href=category/walkthrough.html class=md-nav__link> <span class=md-ellipsis> Walkthrough </span> </a> </li> <li class=md-nav__item> <a href=category/whisper.html class=md-nav__link> <span class=md-ellipsis> Whisper </span> </a> </li> <li class=md-nav__item> <a href=category/langchain.html class=md-nav__link> <span class=md-ellipsis> langchain </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../work.html class=md-nav__link> <span class=md-ellipsis> Work with me </span> </a> </li> <li class=md-nav__item> <a href=../newsletter.html class=md-nav__link> <span class=md-ellipsis> Newsletter </span> </a> </li> <li class=md-nav__item> <a href=../talk.html class=md-nav__link> <span class=md-ellipsis> Talks </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#introduction class=md-nav__link> <span class=md-ellipsis> Introduction </span> </a> </li> <li class=md-nav__item> <a href=#background class=md-nav__link> <span class=md-ellipsis> Background </span> </a> <nav class=md-nav aria-label=Background> <ul class=md-nav__list> <li class=md-nav__item> <a href=#transformers class=md-nav__link> <span class=md-ellipsis> Transformers </span> </a> </li> <li class=md-nav__item> <a href=#attention-and-why-it-sucks class=md-nav__link> <span class=md-ellipsis> Attention and Why it sucks </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#rwkv class=md-nav__link> <span class=md-ellipsis> RWKV </span> </a> <nav class=md-nav aria-label=RWKV> <ul class=md-nav__list> <li class=md-nav__item> <a href=#high-level-understanding class=md-nav__link> <span class=md-ellipsis> High Level Understanding </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#architecture class=md-nav__link> <span class=md-ellipsis> Architecture </span> </a> <nav class=md-nav aria-label=Architecture> <ul class=md-nav__list> <li class=md-nav__item> <a href=#sigmoid class=md-nav__link> <span class=md-ellipsis> Sigmoid </span> </a> </li> <li class=md-nav__item> <a href=#state class=md-nav__link> <span class=md-ellipsis> State </span> </a> </li> <li class=md-nav__item> <a href=#time-mixing class=md-nav__link> <span class=md-ellipsis> Time-Mixing </span> </a> <nav class=md-nav aria-label=Time-Mixing> <ul class=md-nav__list> <li class=md-nav__item> <a href=#inductive-proof class=md-nav__link> <span class=md-ellipsis> Inductive Proof </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#channel-mixing class=md-nav__link> <span class=md-ellipsis> Channel Mixing </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#bringing-it-together class=md-nav__link> <span class=md-ellipsis> Bringing it together </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-content md-content--post" data-md-component=content> <!-- Sidebar --> <div class="md-sidebar md-sidebar--post" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class="md-sidebar__inner md-post"> <nav class="md-nav md-nav--primary"> <!-- Back to overview link --> <div class=md-post__back> <div class="md-nav__title md-nav__container"> <a href=index.html class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> <span class=md-ellipsis> Back to index </span> </a> </div> </div> <div class="md-post__authors md-typeset"> <div class="md-profile md-post__profile"> <span class="md-author md-author--long"> <img src=https://pbs.twimg.com/profile_images/1838778744468836353/utYfioiO_400x400.jpg alt="Ivan Leo"> </span> <span class=md-profile__description> <strong> <a href="https://twitter.com/intent/follow?screen_name=ivanleomk">Ivan Leo</a> </strong> <br> Research Engineer at 567 Labs </span> </div> </div> <!-- Post metadata --> <ul class="md-post__meta md-nav__list"> <li class="md-nav__item md-nav__item--section"> <div class=md-post__title> <span class=md-ellipsis> Metadata </span> </div> <nav class=md-nav> <ul class=md-nav__list> <!-- Post date --> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg> <time datetime="2023-09-28 00:00:00" class=md-ellipsis>September 28, 2023</time> </div> </li> <!-- Post date updated --> <!-- Post categories --> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg> <span class=md-ellipsis> in <a href=category/llms.html>LLMs</a>, <a href=category/rwkv.html>RWKV</a></span> </div> </li> <!-- Post readtime --> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg> <span class=md-ellipsis> 11 min read </span> </div> </li> </ul> </nav> </li> </ul> </nav> <!-- Table of contents, if integrated --> </div> </div> </div> <!-- Page content --> <article class="md-content__inner md-typeset"> <h1>A guide to RWKV V3</h1> <h2 id=introduction>Introduction</h2> <p>RWKV is an alternative to the transformer architecture. It's open source and has it's own <a href=https://arxiv.org/abs/2305.13048>paper</a> over here. I found out about it sometime back in a paper club and thought i'd write a short article about it with what I had learnt.</p> <p>Here are some other resources which you might find useful about RWKVs</p> <ul> <li> <p><a href=https://github.com/PicoCreator/2nd-brain/blob/main/A%20-%20TechTalkCTO/P%20-%20RWKV%20Musings/The%20RWKV%20architecture%20-%20scaling%20RNN%20to%20transformer%20scale/RWKV%20architecture%20-%20Scaling%20an%20RNN%20to%20transformer%20scale.md>RKWV by Picocreator</a> This is a markdown file that was used by one of the contributors - Picocreator to give a short presentation on the RWKV architecture.</p> </li> <li> <p><a href=https://johanwind.github.io/2023/03/23/rwkv_details.html>RKWV in 100 lines</a> Which covers the implementation of RWKV in 100 lines of code. Much of this article is based off the content here - I try to extend and provide my own intuition for some proofs. I've also attached a <a href="https://colab.research.google.com/drive/1ZRHKtJsYY8DSh09Mm2WH7iHayX7NUrMX?usp=sharing">colab notebook</a> for you if you want to play with the code.</p> </li> </ul> <!-- more --> <p>With that being said, let's dive into RWKVs.</p> <blockquote> <p>I'm using the 430M model here. Hence why my embedding space is 1024. Other models might differ so do take note if you are experimenting with the larger models.</p> <p>Do note that there are some issues with numerical instability in the simplified version provided by johanwind which have been fixed in the implementation in Blink's repo. <strong>This is not production code</strong>.</p> </blockquote> <h2 id=background>Background</h2> <h3 id=transformers>Transformers</h3> <blockquote> <p>For a better and more thorough explanation, do check out <a href=https://jaykmody.com/blog/gpt-from-scratch/ >GPT in 60 Lines of numpy</a> by Jay K mody. Some of the content here is paraphrased and I suggest going to the source for the best understanding.</p> </blockquote> <p>What is a transformer? Normally today, when we talk about transformers, we're refering to GPTs - Generative Pre-Trained Transformers. These are huge models that have billions of parameters and are trained on lots of data. This enables them to be able to have an underlying understanding of language.</p> <p>How do they do this? You can think of a transformer as having the following function signature where</p> <ul> <li><code>n_seq</code>: This is the size of the initial data passed in. It has to be less than or equal to the context length.</li> <li><code>n_vocab</code>: Our transformer outputs a prob distribution for each character in the input + the new predicted output.</li> </ul> <div class="language-py highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=k>def</span> <span class=nf>gpt</span><span class=p>(</span><span class=n>inputs</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=nb>int</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=nb>list</span><span class=p>[</span><span class=nb>list</span><span class=p>[</span><span class=nb>float</span><span class=p>]]:</span> <span class=c1># inputs has shape [n_seq]</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a><span class=c1># output has shape [n_seq, n_vocab]</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a>    <span class=n>output</span> <span class=o>=</span> <span class=c1># beep boop neural network magic</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a>    <span class=k>return</span> <span class=n>output</span>
</span></code></pre></div> <p>A big part of this magic happens because of the self-attention mechanism, which gives transformers their special powers but also increases the time needed to make a prediction.</p> <h3 id=attention-and-why-it-sucks>Attention and Why it sucks</h3> <blockquote> <p>Some parts of this explanation are from Code Emporium's Excellent explanation on Transformers which you can view <a href="https://www.youtube.com/watch?v=TQQlZhbC5ps">here</a></p> </blockquote> <p>The most important to take away from this section is that Attention scales quadratically. That's the single biggest bottleneck when we want to scale transformers to do more things.</p> <p>Let's see a graphical representation of what that might look like</p> <p><img alt src=images/attention-equation.png></p> <!-- <img src="/images/attention_illustration.png" alt="Attention" /> --> <p>A few things here to point out</p> <ol> <li> <p>Each word never sees a word that's beyond it's position. In the context of GPT when we are doing next token prediction, we don't the model to "see the future"</p> </li> <li> <p>Every single word prior to a word is compared against it to determine its relevancy to predicting the next word.</p> </li> </ol> <p>In our example above,</p> <ul> <li>What is compared to What (1 comparison )</li> <li>Is is compared to What and Is ( 2 comparisons )</li> <li>My is compared to What,Is and My ( 3 comparisons )</li> <li>Name is compared to What, Is, My and Name ( 4 comparisons )</li> </ul> <blockquote> <p>In some cases, (1) might not hold (Eg. in Translation) where grammar structures differ between languages but for a GPT, this is normally the case.</p> </blockquote> <p>because (2) is happening, then this means that the number of comparisons for a string that has been broken down into <code>n</code> tokens is going to be</p> <div class=arithmatex>\[ 1 + 2 + \dots + n = \frac{n(n+1)}{2} \]</div> <p>which is quadratic in nature. This is an oversimplification of attention but you get the idea. That's why it gets more and more costly as we expand our models context sizes because there are simply more comparisons to be made.</p> <blockquote> <p>We cannot cache these previous lookups in a transformer. This is one of the biggest problems with Transformers that <strong>RWKV doesn't have</strong> as you'll see down below.</p> </blockquote> <p>In transformers, the self-attention mechanism computes query, key, and value vectors for each word in the input sequence and uses the dot product of these vectors to determine the attention scores. This process is sensitive to the specific tokens in the input sequence, and as a result, caching the attention scores for one sequence may not be applicable to a subsequent sequence.</p> <h2 id=rwkv>RWKV</h2> <p>RWKVs aim to solve the issue of attention by approximating attention with a linear operation instead. This results in 1-1.5x cheaper training costs and around 10x cheaper inference costs, especially as the number of tokens passed into the model increase over time.</p> <p>We can see this with the following graphic from the RWKV paper that shows how the inference cost grows with respect to the token count. This means that as the length of what we pass in increases, we enjoy significantly lower inference time with the RWKV architecture.</p> <p><img alt src=images/Scaling_RWKV.png></p> <h3 id=high-level-understanding>High Level Understanding</h3> <p>RWKVs operate using two main mechanisms</p> <ol> <li>A world state : This stores information on information such as previous computations</li> <li>Temporal mechanism : RWKV uses a decay function to reduce the weightage of past information w.r.t new information.</li> </ol> <p>However, as a result of a world state, the RWKV model has a major weakness - it might end up discarding content which is relevant but not explictly requested at the start of the prompt.</p> <p>As a result, when we're prompting the RWKV model, we want to use the following syntax</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a>{{INSTRUCTION}}
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a>{{CONTEXT}}
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a>{{ANSWER}}
</span></code></pre></div> <p>instead of the typical format of</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a>{{CONTEXT}}
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a>{{INSTRUCTION}}
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a>{{ANSWER}}
</span></code></pre></div> <h2 id=architecture>Architecture</h2> <h3 id=sigmoid>Sigmoid</h3> <p>Intuitively, the sigmoid function is used as a activation function in both the time-mixing and acts as a forget gate in our Time Mixing and Channel Mixing blocks. Since all values are effectively coerced from 0 to 1, A value closer to 0 means the information should be forgotten, while a value closer to 1 means the information should be retained.</p> <p>This plays a crucial role in determining the relevance of information from prior steps, allowing the network to selectively retain or discard information based on the current input and previous hidden state</p> <h3 id=state>State</h3> <p>In essence, we can think of the state as being comprised of the following components</p> <p><code>[layerState,layerState,.... ]</code></p> <p>Inside each layerState, we have a total of 4 subarrays</p> <p><code>state = np.zeros((N_LAYER, 4, N_EMBD), dtype=np.float32)</code></p> <p>inside the state, we allocate space for each element as</p> <ul> <li>0 : prevX computation</li> <li>1 : prevNum Computation</li> <li>2 : prevDen Computation</li> <li>3 : prevChannelMixing result</li> </ul> <p>This helps our model to be able to get some concept of time. Note that (3) is mostly used to replace the short-term memory ( since it can only look back a single step )</p> <h3 id=time-mixing>Time-Mixing</h3> <p>Time mixing approximates attention and can be likened to the LSTM component of a traditional RNN architecture. This is because it has access to two things</p> <ul> <li>Previous World State</li> <li>Learned weights to determine how to combine previous computations and new computations.</li> </ul> <blockquote> <p>Intuitively as time <code>t</code> increases, then the vector is dependent on a long history and a summation of an increasing number of terms. This creates a memory which can keep track of information provided in an earlier context.</p> </blockquote> <p>We can visualise the entire architecture of the time-mixing block as seen below.</p> <p><img alt src=images/Time_Mixing_Block.png></p> <p>Let's look at a function to compute time-mixing</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=c1># Time mix layer with the various params</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a><span class=k>def</span> <span class=nf>time_mixing</span><span class=p>(</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>        <span class=c1># Incoming state of the current token</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>        <span class=n>x</span><span class=p>,</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a>        <span class=c1># Previous token shift state</span>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a>        <span class=n>last_x</span><span class=p>,</span>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a>        <span class=c1># Previous state, split across 2 values to prevent overflows</span>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a>        <span class=n>last_num</span><span class=p>,</span> <span class=n>last_den</span><span class=p>,</span>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a>        <span class=c1># Various weights, all trainable</span>
</span><span id=__span-3-10><a id=__codelineno-3-10 name=__codelineno-3-10 href=#__codelineno-3-10></a>        <span class=n>decay</span><span class=p>,</span> <span class=n>bonus</span><span class=p>,</span> <span class=n>mix_k</span><span class=p>,</span> <span class=n>mix_v</span><span class=p>,</span> <span class=n>mix_r</span><span class=p>,</span> <span class=n>Wk</span><span class=p>,</span> <span class=n>Wv</span><span class=p>,</span> <span class=n>Wr</span><span class=p>,</span> <span class=n>Wout</span>
</span><span id=__span-3-11><a id=__codelineno-3-11 name=__codelineno-3-11 href=#__codelineno-3-11></a>    <span class=p>):</span>
</span><span id=__span-3-12><a id=__codelineno-3-12 name=__codelineno-3-12 href=#__codelineno-3-12></a>
</span><span id=__span-3-13><a id=__codelineno-3-13 name=__codelineno-3-13 href=#__codelineno-3-13></a>    <span class=c1># Given the incoming state, and the previous token shift state</span>
</span><span id=__span-3-14><a id=__codelineno-3-14 name=__codelineno-3-14 href=#__codelineno-3-14></a>    <span class=c1># compute R,K,V values. The `x * mix + last * (1-mix)` pattern</span>
</span><span id=__span-3-15><a id=__codelineno-3-15 name=__codelineno-3-15 href=#__codelineno-3-15></a>    <span class=c1># helps the model have trained weights to decide which factors</span>
</span><span id=__span-3-16><a id=__codelineno-3-16 name=__codelineno-3-16 href=#__codelineno-3-16></a>    <span class=c1># it wants to use for the respective process</span>
</span><span id=__span-3-17><a id=__codelineno-3-17 name=__codelineno-3-17 href=#__codelineno-3-17></a>    <span class=n>k</span> <span class=o>=</span> <span class=n>Wk</span> <span class=o>@</span> <span class=p>(</span> <span class=n>x</span> <span class=o>*</span> <span class=n>mix_k</span> <span class=o>+</span> <span class=n>last_x</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>mix_k</span><span class=p>)</span> <span class=p>)</span>
</span><span id=__span-3-18><a id=__codelineno-3-18 name=__codelineno-3-18 href=#__codelineno-3-18></a>    <span class=n>v</span> <span class=o>=</span> <span class=n>Wv</span> <span class=o>@</span> <span class=p>(</span> <span class=n>x</span> <span class=o>*</span> <span class=n>mix_v</span> <span class=o>+</span> <span class=n>last_x</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>mix_v</span><span class=p>)</span> <span class=p>)</span>
</span><span id=__span-3-19><a id=__codelineno-3-19 name=__codelineno-3-19 href=#__codelineno-3-19></a>
</span><span id=__span-3-20><a id=__codelineno-3-20 name=__codelineno-3-20 href=#__codelineno-3-20></a>    <span class=c1># Since R is used for the final gating of output (similar to attention score)</span>
</span><span id=__span-3-21><a id=__codelineno-3-21 name=__codelineno-3-21 href=#__codelineno-3-21></a>    <span class=c1># you can view this as a lite form of Q @ K</span>
</span><span id=__span-3-22><a id=__codelineno-3-22 name=__codelineno-3-22 href=#__codelineno-3-22></a>    <span class=n>r</span> <span class=o>=</span> <span class=n>Wr</span> <span class=o>@</span> <span class=p>(</span> <span class=n>x</span> <span class=o>*</span> <span class=n>mix_r</span> <span class=o>+</span> <span class=n>last_x</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>mix_r</span><span class=p>)</span> <span class=p>)</span>
</span><span id=__span-3-23><a id=__codelineno-3-23 name=__codelineno-3-23 href=#__codelineno-3-23></a>
</span><span id=__span-3-24><a id=__codelineno-3-24 name=__codelineno-3-24 href=#__codelineno-3-24></a>    <span class=c1># Here we effectively do magic(last_state + k) * v</span>
</span><span id=__span-3-25><a id=__codelineno-3-25 name=__codelineno-3-25 href=#__codelineno-3-25></a>    <span class=c1>#</span>
</span><span id=__span-3-26><a id=__codelineno-3-26 name=__codelineno-3-26 href=#__codelineno-3-26></a>    <span class=c1># But in essence, its the &quot;summation with decay&quot; of all the past expotents</span>
</span><span id=__span-3-27><a id=__codelineno-3-27 name=__codelineno-3-27 href=#__codelineno-3-27></a>    <span class=c1># divided by another set of expotents for numeric stability</span>
</span><span id=__span-3-28><a id=__codelineno-3-28 name=__codelineno-3-28 href=#__codelineno-3-28></a>    <span class=c1># (aka anti exploding gradients).</span>
</span><span id=__span-3-29><a id=__codelineno-3-29 name=__codelineno-3-29 href=#__codelineno-3-29></a>    <span class=c1>#</span>
</span><span id=__span-3-30><a id=__codelineno-3-30 name=__codelineno-3-30 href=#__codelineno-3-30></a>    <span class=c1># Bonus is used to boost the signal for the WKV value</span>
</span><span id=__span-3-31><a id=__codelineno-3-31 name=__codelineno-3-31 href=#__codelineno-3-31></a>    <span class=n>wkv</span> <span class=o>=</span> <span class=p>(</span><span class=n>last_num</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=n>bonus</span> <span class=o>+</span> <span class=n>k</span><span class=p>)</span> <span class=o>*</span> <span class=n>v</span><span class=p>)</span> <span class=o>/</span> \
</span><span id=__span-3-32><a id=__codelineno-3-32 name=__codelineno-3-32 href=#__codelineno-3-32></a>          <span class=p>(</span><span class=n>last_den</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=n>bonus</span> <span class=o>+</span> <span class=n>k</span><span class=p>))</span>
</span><span id=__span-3-33><a id=__codelineno-3-33 name=__codelineno-3-33 href=#__codelineno-3-33></a>
</span><span id=__span-3-34><a id=__codelineno-3-34 name=__codelineno-3-34 href=#__codelineno-3-34></a>    <span class=c1># We compute the cumulative sum, for the next round, where it</span>
</span><span id=__span-3-35><a id=__codelineno-3-35 name=__codelineno-3-35 href=#__codelineno-3-35></a>    <span class=c1># is stored and forwarded as a state, with a gradual</span>
</span><span id=__span-3-36><a id=__codelineno-3-36 name=__codelineno-3-36 href=#__codelineno-3-36></a>    <span class=c1># decay value on the previous value.</span>
</span><span id=__span-3-37><a id=__codelineno-3-37 name=__codelineno-3-37 href=#__codelineno-3-37></a>    <span class=c1>#</span>
</span><span id=__span-3-38><a id=__codelineno-3-38 name=__codelineno-3-38 href=#__codelineno-3-38></a>    <span class=c1># `exp(-exp(decay))` is essentialy a math hack to ensure decay</span>
</span><span id=__span-3-39><a id=__codelineno-3-39 name=__codelineno-3-39 href=#__codelineno-3-39></a>    <span class=c1># is between 0-1, and will gradually fade out past value if desired</span>
</span><span id=__span-3-40><a id=__codelineno-3-40 name=__codelineno-3-40 href=#__codelineno-3-40></a>    <span class=c1>#</span>
</span><span id=__span-3-41><a id=__codelineno-3-41 name=__codelineno-3-41 href=#__codelineno-3-41></a>    <span class=c1># `exp(k) / exp(k) * v` is the summation accumulation of the current state</span>
</span><span id=__span-3-42><a id=__codelineno-3-42 name=__codelineno-3-42 href=#__codelineno-3-42></a>    <span class=c1># to be used in the next time mix</span>
</span><span id=__span-3-43><a id=__codelineno-3-43 name=__codelineno-3-43 href=#__codelineno-3-43></a>    <span class=n>num</span> <span class=o>=</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>exp</span><span class=p>(</span><span class=n>decay</span><span class=p>))</span> <span class=o>*</span> <span class=n>last_num</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=n>k</span><span class=p>)</span> <span class=o>*</span> <span class=n>v</span>
</span><span id=__span-3-44><a id=__codelineno-3-44 name=__codelineno-3-44 href=#__codelineno-3-44></a>    <span class=n>den</span> <span class=o>=</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>exp</span><span class=p>(</span><span class=n>decay</span><span class=p>))</span> <span class=o>*</span> <span class=n>last_den</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=n>k</span><span class=p>)</span>
</span><span id=__span-3-45><a id=__codelineno-3-45 name=__codelineno-3-45 href=#__codelineno-3-45></a>
</span><span id=__span-3-46><a id=__codelineno-3-46 name=__codelineno-3-46 href=#__codelineno-3-46></a>    <span class=c1># sigmoid then acts looseley as both a part of Q in QKV,</span>
</span><span id=__span-3-47><a id=__codelineno-3-47 name=__codelineno-3-47 href=#__codelineno-3-47></a>    <span class=c1># and as a forget gate in LSTM (together with decay)</span>
</span><span id=__span-3-48><a id=__codelineno-3-48 name=__codelineno-3-48 href=#__codelineno-3-48></a>    <span class=c1># for the WKV values</span>
</span><span id=__span-3-49><a id=__codelineno-3-49 name=__codelineno-3-49 href=#__codelineno-3-49></a>    <span class=n>rwkv</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>r</span><span class=p>)</span> <span class=o>*</span> <span class=n>wkv</span>
</span><span id=__span-3-50><a id=__codelineno-3-50 name=__codelineno-3-50 href=#__codelineno-3-50></a>
</span><span id=__span-3-51><a id=__codelineno-3-51 name=__codelineno-3-51 href=#__codelineno-3-51></a>    <span class=c1># And finally that gets normalized into an output, and next state</span>
</span><span id=__span-3-52><a id=__codelineno-3-52 name=__codelineno-3-52 href=#__codelineno-3-52></a>    <span class=k>return</span> <span class=n>Wout</span> <span class=o>@</span> <span class=n>rwkv</span><span class=p>,</span> <span class=p>(</span><span class=n>x</span><span class=p>,</span><span class=n>num</span><span class=p>,</span><span class=n>den</span><span class=p>)</span>
</span></code></pre></div> <p>This function implements the following equations of</p> <p><img alt src=images/time_mixing_eqn.png></p> <p>In our function, We have the parameters of</p> <ul> <li> <p><code>last_x,last_num, last_den</code> : These are all stored in a huge <code>(N_Layers, 4,N_embeddings)</code> array and simply store the previous computations for each layer.</p> </li> <li> <p><code>last_x</code>: <span class=arithmatex>\(x_{t-1}\)</span></p> </li> <li><code>last_num</code>: <span class=arithmatex>\(wkv_{t-1}\)</span> numerator</li> <li> <p><code>last_den</code>: <span class=arithmatex>\(wkv_{t-1}\)</span> denominator</p> </li> <li> <p><code>decay, bonus, mix_k, mix_v, mix_r, Wk, Wv, Wr, Wout</code></p> </li> <li><code>decay</code> : <span class=arithmatex>\(w\)</span> parameter can be treated as <span class=arithmatex>\(e^{-\text{decay}}\)</span></li> <li><code>bonus</code> : This is equivalent to the <code>u</code> parameter above</li> <li><code>mix_k</code>,<code>mix_r</code> and <code>mix_v</code> are equal to <span class=arithmatex>\(\mu_k,\mu_r,\mu_t\)</span></li> <li><code>Wk,Wv,Wr and wout</code> are equivalent to <span class=arithmatex>\(W_k,W_v,W_r\)</span> and <span class=arithmatex>\(W_o\)</span> above</li> <li>Note here that <span class=arithmatex>\(\sigma\)</span> simply represents the sigmoid activation function.</li> </ul> <p>In the code above, it might be a little bit difficult to visualise the dimensions so</p> <ul> <li>(1024,1024) : <span class=arithmatex>\(W_r, W_k, W_v\)</span></li> <li>(1024, ): <span class=arithmatex>\(\mu_r,x_t,x_{t-1},\mu_k,\mu_v\)</span> , decay, bonus</li> </ul> <p>This means that essentially <span class=arithmatex>\(r_t,k_t\)</span> and <span class=arithmatex>\(v_t\)</span> are all going to be of the dimension <code>(1024,)</code> and the final output of this function will be of <code>(1024,)</code>.</p> <blockquote> <p>What's interesting about this is that information on all previous tokens seen is stored in a 1024 dimensional array. Compare this to transformers that generate a <code>n_seq x n_embd</code> array due to the attention mechanism. Is attention truly even needed?</p> </blockquote> <h4 id=inductive-proof>Inductive Proof</h4> <p>I was slightly confused by these 4 lines of code since I couldn't quite understand the leap from last_num to num and den to last_den.</p> <div class="language-py highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a><span class=n>wkv</span> <span class=o>=</span> <span class=p>(</span><span class=n>last_num</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=n>bonus</span> <span class=o>+</span> <span class=n>k</span><span class=p>)</span> <span class=o>*</span> <span class=n>v</span><span class=p>)</span> <span class=o>/</span>      \
</span><span id=__span-4-2><a id=__codelineno-4-2 name=__codelineno-4-2 href=#__codelineno-4-2></a>          <span class=p>(</span><span class=n>last_den</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=n>bonus</span> <span class=o>+</span> <span class=n>k</span><span class=p>))</span>
</span><span id=__span-4-3><a id=__codelineno-4-3 name=__codelineno-4-3 href=#__codelineno-4-3></a><span class=n>rwkv</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>r</span><span class=p>)</span> <span class=o>*</span> <span class=n>wkv</span>
</span><span id=__span-4-4><a id=__codelineno-4-4 name=__codelineno-4-4 href=#__codelineno-4-4></a>
</span><span id=__span-4-5><a id=__codelineno-4-5 name=__codelineno-4-5 href=#__codelineno-4-5></a><span class=n>num</span> <span class=o>=</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>exp</span><span class=p>(</span><span class=n>decay</span><span class=p>))</span> <span class=o>*</span> <span class=n>last_num</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=n>k</span><span class=p>)</span> <span class=o>*</span> <span class=n>v</span>
</span><span id=__span-4-6><a id=__codelineno-4-6 name=__codelineno-4-6 href=#__codelineno-4-6></a><span class=n>den</span> <span class=o>=</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>exp</span><span class=p>(</span><span class=n>decay</span><span class=p>))</span> <span class=o>*</span> <span class=n>last_den</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=n>k</span><span class=p>)</span>
</span></code></pre></div> <p>Which tries to implement the step of</p> <div class=arithmatex>\[ wkv_t=\frac{\sum_{i=1}^{t-1}e^{-(t-1-i)w+k_i}v_{i} + e^{\mu+k_t}v_t}{\sum_{i=1}^{t-1}e^{-(t-1-i)w+k_i} + e^{\mu+k_t}} \]</div> <p>Let's try to build the intuition for this step by step. Let's say we wanted to take the sum of all the elements in the following sum. ( This is present in the numerator ).</p> <div class=arithmatex>\[ \sum_{i=1}^{t-1}e^{-(t-1-i)w+k_i}v_{i} \]</div> <p>Let's also designate the sum of all the elements defined in the sum as <span class=arithmatex>\(\alpha_{t-1}\)</span> where we are summing elements from 1 to <span class=arithmatex>\(t-1\)</span>. How do we get from <span class=arithmatex>\(\alpha_{t-1}\)</span> to <span class=arithmatex>\(\alpha_{t}\)</span>?</p> <p>If we look at the term <span class=arithmatex>\(\alpha_{t-1}\)</span>, we can see that the final value of the summation is going to be</p> <div class=arithmatex>\[ e^{-(t-1-(t-1))w+k_i} = e^{k_{t-i}} \]</div> <p>Well, what about the second last value of the summation then? it's going to be</p> <div class=arithmatex>\[ e^{-(t-1-(t-2))w+k_{t-2}} = e^{-w + k_{t-i}} \]</div> <p>This means that we can therefore link <span class=arithmatex>\(\alpha_{t-1}\)</span> with <span class=arithmatex>\(\alpha_{t}\)</span> by doing</p> <div class=arithmatex>\[ \sum_{i=1}^{t-1}e^{-(t-1-i)w+k_i}v_{i} = \sum_{i=1}^{t-2}e^{-(t-1-i)w+k_i}v_{i} + e^{k_{t-1}} \]</div> <p>We can perform a similar substituition to get <span class=arithmatex>\(\beta_{t} =\beta_{t-1}+ e^{k_{t-1}}\)</span>.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a><span class=n>num</span> <span class=o>=</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>exp</span><span class=p>(</span><span class=n>decay</span><span class=p>))</span> <span class=o>*</span> <span class=n>last_num</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=n>k</span><span class=p>)</span> <span class=o>*</span> <span class=n>v</span>
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a><span class=n>den</span> <span class=o>=</span> <span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=n>exp</span><span class=p>(</span><span class=n>decay</span><span class=p>))</span> <span class=o>*</span> <span class=n>last_den</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=n>k</span><span class=p>)</span>
</span></code></pre></div> <p>We can then rewrite our equation for <span class=arithmatex>\(wkv_{t}\)</span> as</p> <div class=arithmatex>\[ wkv_{t} = \frac{\alpha_{t-1} + e^{\text{bonus} + k_{t}}v_t }{\beta_{t-1} + e^{\text{bonus} + k_{t}}} \]</div> <p>which corresponds to our implementation of</p> <div class="language-py highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a><span class=n>wkv</span> <span class=o>=</span> <span class=p>(</span><span class=n>last_num</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=n>bonus</span> <span class=o>+</span> <span class=n>k</span><span class=p>)</span> <span class=o>*</span> <span class=n>v</span><span class=p>)</span> <span class=o>/</span>      \
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a>          <span class=p>(</span><span class=n>last_den</span> <span class=o>+</span> <span class=n>exp</span><span class=p>(</span><span class=n>bonus</span> <span class=o>+</span> <span class=n>k</span><span class=p>))</span>
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>    <span class=n>rwkv</span> <span class=o>=</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>r</span><span class=p>)</span> <span class=o>*</span> <span class=n>wkv</span>
</span></code></pre></div> <h3 id=channel-mixing>Channel Mixing</h3> <p>The channel mixing is the short term component of the system - it only has access to the previous value and it takes a weighted sum of the previous and current value that it has calculated.</p> <p><img alt src=images/Channel_Mixing.png></p> <p>It's significantly easier to understand. <strong>Note that all these parameters are learned parameters with the exception of the <span class=arithmatex>\(x_{t-1}\)</span> values</strong>.</p> <p>Our channel mixing function is implemented as</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a><span class=k>def</span> <span class=nf>channel_mixing</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>last_x</span><span class=p>,</span> <span class=n>mix_k</span><span class=p>,</span> <span class=n>mix_r</span><span class=p>,</span> <span class=n>Wk</span><span class=p>,</span> <span class=n>Wr</span><span class=p>,</span> <span class=n>Wv</span><span class=p>):</span>
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>    <span class=n>k</span> <span class=o>=</span> <span class=n>Wk</span> <span class=o>@</span> <span class=p>(</span> <span class=n>x</span> <span class=o>*</span> <span class=n>mix_k</span> <span class=o>+</span> <span class=n>last_x</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>mix_k</span><span class=p>)</span> <span class=p>)</span>
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a>    <span class=n>r</span> <span class=o>=</span> <span class=n>Wr</span> <span class=o>@</span> <span class=p>(</span> <span class=n>x</span> <span class=o>*</span> <span class=n>mix_r</span> <span class=o>+</span> <span class=n>last_x</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>mix_r</span><span class=p>)</span> <span class=p>)</span>
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a>    <span class=n>vk</span> <span class=o>=</span> <span class=n>Wv</span> <span class=o>@</span> <span class=n>np</span><span class=o>.</span><span class=n>maximum</span><span class=p>(</span><span class=n>k</span><span class=p>,</span> <span class=mi>0</span><span class=p>)</span><span class=o>**</span><span class=mi>2</span>
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a>    <span class=k>return</span> <span class=n>sigmoid</span><span class=p>(</span><span class=n>r</span><span class=p>)</span> <span class=o>*</span> <span class=n>vk</span><span class=p>,</span> <span class=n>x</span>
</span></code></pre></div> <ul> <li>It's interesting here to note that</li> <li><span class=arithmatex>\(w_k\)</span> has a dimensionality of <span class=arithmatex>\([4096,1024]\)</span></li> <li><span class=arithmatex>\(w_v\)</span> has a dimensionality of <span class=arithmatex>\([1024,4096]\)</span></li> </ul> <p>This means that we perform a scaling in the channel_mixing step from an initial dimensionality of 1024 to a dimensionality of 4096. This is very similar to the feed forward network that is used as a transformer.</p> <h2 id=bringing-it-together>Bringing it together</h2> <p>If we look at the code for RWKV in 100 lines, we notice that the RWKV architecture for the 400M model comprises 24 layers of the same block.</p> <p>Each block comprises</p> <ul> <li>1 initial layer norm ( Constant work since the size of the input is fixed as a (1024,) matrix )</li> <li>1 time_mixing step</li> <li>1 layer norm ( Also fixed size input of (1024,) )</li> <li>1 channel mixing step</li> </ul> <p>All of the operations that we perform in the time_mixing and channel_mixing step are all going to linearly scale with the size of our context.</p> <p>The way I understand it is that since information on all previous token is compressed into a single token value</p> <ul> <li>Time Mixing requires a constant number of operations per prediction step</li> <li>Channel Mixing also requires a constant number of operations per prediction step</li> </ul> <p>Therefore, for every additional token we want to predict or want to add into our context, we have a constant amount of work to be done for each of these tokens. Therefore, this means that the amount of computational work we need to perform will scale roughly linearly with the amount of tokens we eventually need to predict/proccess.</p> <!-- Add social sharing buttons --> <div class=md-social-share style="margin: 2rem 0; text-align: center"> <span class=md-social-share__label style="display: block; margin-bottom: 1rem; font-weight: 500">Share this post:</span> <div class=md-social-share__buttons style="display: flex; gap: 1rem; justify-content: center"> <a href="https://x.com/intent/tweet?text=A%20guide%20to%20RWKV%20V3&url=https://ivanleo.com/blog/a-guide-to-rwkv-v3.html" class=md-social-share__button style="
            display: inline-flex;
            align-items: center;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            background: #000;
            color: #fff;
            text-decoration: none;
            transition: opacity 0.2s;
          " target=_blank rel=noopener> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"/></svg> <span style="margin-left: 0.5rem">Share on X</span> </a> <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://ivanleo.com/blog/a-guide-to-rwkv-v3.html&title=A%20guide%20to%20RWKV%20V3&summary=&source=" class=md-social-share__button style="
            display: inline-flex;
            align-items: center;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            background: #0077b5;
            color: #fff;
            text-decoration: none;
            transition: opacity 0.2s;
          " target=_blank rel=noopener> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.06 2.06 0 0 1-2.063-2.065 2.064 2.064 0 1 1 2.063 2.065m1.782 13.019H3.555V9h3.564zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0z"/></svg> <span style="margin-left: 0.5rem">Share on LinkedIn</span> </a> </div> </div> <div class=newsletter-section style="margin: 2rem 0"> <script async data-uid=b184c2f91e src=https://ivan-leo.kit.com/b184c2f91e/index.js></script> </div> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["content.code.copy", "content.code.annotate", "navigation.tabs", "toc.follow", "navigation.path"], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../assets/javascripts/bundle.83f73b43.min.js></script> <script src=../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>