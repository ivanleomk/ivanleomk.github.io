<!--
  Copyright (c) 2016-2024 Martin Donath <martin.donath@squidfunk.com>

  Permission is hereby granted, free of charge, to any person obtaining a copy
  of this software and associated documentation files (the "Software"), to
  deal in the Software without restriction, including without limitation the
  rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
  sell copies of the Software, and to permit persons to whom the Software is
  furnished to do so, subject to the following conditions:

  The above copyright notice and this permission notice shall be included in
  all copies or substantial portions of the Software.

  THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
  IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
  FITNESS FOR A PARTICULAR PURPOSE AND NON-INFRINGEMENT. IN NO EVENT SHALL THE
  AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
  LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
  FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
  IN THE SOFTWARE.
--> <!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Speedrunning everything I learnt in the past year"><link rel=canonical href=https://ivanleo.com/blog/grokking-llms.html><link rel=prev href=writing-your-first-rust-cli-tool.html><link rel=next href=ai-engineering-world-fair.html><link rel=alternate type=application/rss+xml title="RSS feed" href=../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../feed_rss_updated.xml><link rel=icon href=../assets/images/favicon.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.5.44"><title>Grokking LLMs - Ivan's Blog</title><link rel=stylesheet href=../assets/stylesheets/main.0253249f.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../assets/_mkdocstrings.css><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-MM8QMY5JWN"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-MM8QMY5JWN",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-MM8QMY5JWN",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><meta property=og:type content=website><meta property=og:title content="Grokking LLMs - Ivan's Blog"><meta property=og:description content="Speedrunning everything I learnt in the past year"><meta property=og:image content=https://ivanleo.com/assets/images/social/blog/posts/grokking-llms.png><meta property=og:image:type content=image/png><meta property=og:image:width content=1200><meta property=og:image:height content=630><meta property=og:url content=https://ivanleo.com/blog/grokking-llms.html><meta name=twitter:card content=summary_large_image><meta name=twitter:title content="Grokking LLMs - Ivan's Blog"><meta name=twitter:description content="Speedrunning everything I learnt in the past year"><meta name=twitter:image content=https://ivanleo.com/assets/images/social/blog/posts/grokking-llms.png></head> <body dir=ltr> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#some-background class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../index.html title="Ivan's Blog" class="md-header__button md-logo" aria-label="Ivan's Blog" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Ivan's Blog </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Grokking LLMs </span> </div> </div> </div> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> </label> <nav class=md-search__options aria-label=Search> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg> </button> </nav> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../index.html class=md-tabs__link> About me </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=index.html class=md-tabs__link> Blog </a> </li> <li class=md-tabs__item> <a href=../work.html class=md-tabs__link> Work with me </a> </li> <li class=md-tabs__item> <a href=../newsletter.html class=md-tabs__link> Newsletter </a> </li> <li class=md-tabs__item> <a href=../talk.html class=md-tabs__link> Talks </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../index.html title="Ivan's Blog" class="md-nav__button md-logo" aria-label="Ivan's Blog" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg> </a> Ivan's Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../index.html class=md-nav__link> <span class=md-ellipsis> About me </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <label class=md-nav__link for=__nav_2 id=__nav_2_label tabindex> <span class=md-ellipsis> Blog </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--active"> <a href=index.html class=md-nav__link> <span class=md-ellipsis> Index </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex=0> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=archive/2025.html class=md-nav__link> <span class=md-ellipsis> 2025 </span> </a> </li> <li class=md-nav__item> <a href=archive/2024.html class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> <li class=md-nav__item> <a href=archive/2023.html class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex=0> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=category/ai.html class=md-nav__link> <span class=md-ellipsis> AI </span> </a> </li> <li class=md-nav__item> <a href=category/ai-engineering.html class=md-nav__link> <span class=md-ellipsis> AI Engineering </span> </a> </li> <li class=md-nav__item> <a href=category/advice.html class=md-nav__link> <span class=md-ellipsis> Advice </span> </a> </li> <li class=md-nav__item> <a href=category/applied-ai.html class=md-nav__link> <span class=md-ellipsis> Applied AI </span> </a> </li> <li class=md-nav__item> <a href=category/braintrust.html class=md-nav__link> <span class=md-ellipsis> Braintrust </span> </a> </li> <li class=md-nav__item> <a href=category/career.html class=md-nav__link> <span class=md-ellipsis> Career </span> </a> </li> <li class=md-nav__item> <a href=category/clustering.html class=md-nav__link> <span class=md-ellipsis> Clustering </span> </a> </li> <li class=md-nav__item> <a href=category/comfyui.html class=md-nav__link> <span class=md-ellipsis> ComfyUI </span> </a> </li> <li class=md-nav__item> <a href=category/debugging.html class=md-nav__link> <span class=md-ellipsis> Debugging </span> </a> </li> <li class=md-nav__item> <a href=category/deployment.html class=md-nav__link> <span class=md-ellipsis> Deployment </span> </a> </li> <li class=md-nav__item> <a href=category/diffusion-models.html class=md-nav__link> <span class=md-ellipsis> Diffusion Models </span> </a> </li> <li class=md-nav__item> <a href=category/documentation.html class=md-nav__link> <span class=md-ellipsis> Documentation </span> </a> </li> <li class=md-nav__item> <a href=category/evals.html class=md-nav__link> <span class=md-ellipsis> Evals </span> </a> </li> <li class=md-nav__item> <a href=category/evaluation.html class=md-nav__link> <span class=md-ellipsis> Evaluation </span> </a> </li> <li class=md-nav__item> <a href=category/evaluations.html class=md-nav__link> <span class=md-ellipsis> Evaluations </span> </a> </li> <li class=md-nav__item> <a href=category/instructor.html class=md-nav__link> <span class=md-ellipsis> Instructor </span> </a> </li> <li class=md-nav__item> <a href=category/llm.html class=md-nav__link> <span class=md-ellipsis> LLM </span> </a> </li> <li class=md-nav__item> <a href=category/llms.html class=md-nav__link> <span class=md-ellipsis> LLMs </span> </a> </li> <li class=md-nav__item> <a href=category/mcps.html class=md-nav__link> <span class=md-ellipsis> MCPs </span> </a> </li> <li class=md-nav__item> <a href=category/mac.html class=md-nav__link> <span class=md-ellipsis> Mac </span> </a> </li> <li class=md-nav__item> <a href=category/machine-learning.html class=md-nav__link> <span class=md-ellipsis> Machine Learning </span> </a> </li> <li class=md-nav__item> <a href=category/modal.html class=md-nav__link> <span class=md-ellipsis> Modal </span> </a> </li> <li class=md-nav__item> <a href=category/personal.html class=md-nav__link> <span class=md-ellipsis> Personal </span> </a> </li> <li class=md-nav__item> <a href=category/personal-development.html class=md-nav__link> <span class=md-ellipsis> Personal Development </span> </a> </li> <li class=md-nav__item> <a href=category/python.html class=md-nav__link> <span class=md-ellipsis> Python </span> </a> </li> <li class=md-nav__item> <a href=category/rag.html class=md-nav__link> <span class=md-ellipsis> RAG </span> </a> </li> <li class=md-nav__item> <a href=category/rwkv.html class=md-nav__link> <span class=md-ellipsis> RWKV </span> </a> </li> <li class=md-nav__item> <a href=category/synthetic-data.html class=md-nav__link> <span class=md-ellipsis> Synthetic Data </span> </a> </li> <li class=md-nav__item> <a href=category/testing.html class=md-nav__link> <span class=md-ellipsis> Testing </span> </a> </li> <li class=md-nav__item> <a href=category/trends.html class=md-nav__link> <span class=md-ellipsis> Trends </span> </a> </li> <li class=md-nav__item> <a href=category/ui-generation.html class=md-nav__link> <span class=md-ellipsis> UI Generation </span> </a> </li> <li class=md-nav__item> <a href=category/uiux.html class=md-nav__link> <span class=md-ellipsis> UI/UX </span> </a> </li> <li class=md-nav__item> <a href=category/voice.html class=md-nav__link> <span class=md-ellipsis> Voice </span> </a> </li> <li class=md-nav__item> <a href=category/walkthrough.html class=md-nav__link> <span class=md-ellipsis> Walkthrough </span> </a> </li> <li class=md-nav__item> <a href=category/whisper.html class=md-nav__link> <span class=md-ellipsis> Whisper </span> </a> </li> <li class=md-nav__item> <a href=category/langchain.html class=md-nav__link> <span class=md-ellipsis> langchain </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../work.html class=md-nav__link> <span class=md-ellipsis> Work with me </span> </a> </li> <li class=md-nav__item> <a href=../newsletter.html class=md-nav__link> <span class=md-ellipsis> Newsletter </span> </a> </li> <li class=md-nav__item> <a href=../talk.html class=md-nav__link> <span class=md-ellipsis> Talks </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#quick-definitions class=md-nav__link> <span class=md-ellipsis> Quick Definitions </span> </a> <nav class=md-nav aria-label="Quick Definitions"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#deep-learning class=md-nav__link> <span class=md-ellipsis> Deep Learning </span> </a> </li> <li class=md-nav__item> <a href=#rnns-and-lstms class=md-nav__link> <span class=md-ellipsis> RNNs and LSTMs </span> </a> </li> <li class=md-nav__item> <a href=#attention class=md-nav__link> <span class=md-ellipsis> Attention </span> </a> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-content md-content--post" data-md-component=content> <!-- Sidebar --> <div class="md-sidebar md-sidebar--post" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class="md-sidebar__inner md-post"> <nav class="md-nav md-nav--primary"> <!-- Back to overview link --> <div class=md-post__back> <div class="md-nav__title md-nav__container"> <a href=index.html class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg> <span class=md-ellipsis> Back to index </span> </a> </div> </div> <div class="md-post__authors md-typeset"> <div class="md-profile md-post__profile"> <span class="md-author md-author--long"> <img src=https://pbs.twimg.com/profile_images/1838778744468836353/utYfioiO_400x400.jpg alt="Ivan Leo"> </span> <span class=md-profile__description> <strong> <a href="https://twitter.com/intent/follow?screen_name=ivanleomk">Ivan Leo</a> </strong> <br> Research Engineer at 567 Labs </span> </div> </div> <!-- Post metadata --> <ul class="md-post__meta md-nav__list"> <li class="md-nav__item md-nav__item--section"> <div class=md-post__title> <span class=md-ellipsis> Metadata </span> </div> <nav class=md-nav> <ul class=md-nav__list> <!-- Post date --> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 19H5V8h14m-3-7v2H8V1H6v2H5c-1.11 0-2 .89-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2h-1V1m-1 11h-5v5h5z"/></svg> <time datetime="2024-05-02 00:00:00" class=md-ellipsis>May 2, 2024</time> </div> </li> <!-- Post date updated --> <!-- Post categories --> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9 3v15h3V3zm3 2 4 13 3-1-4-13zM5 5v13h3V5zM3 19v2h18v-2z"/></svg> <span class=md-ellipsis> in <a href=category/llms.html>LLMs</a>, <a href=category/walkthrough.html>Walkthrough</a></span> </div> </li> <!-- Post readtime --> <li class=md-nav__item> <div class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20a8 8 0 0 0 8-8 8 8 0 0 0-8-8 8 8 0 0 0-8 8 8 8 0 0 0 8 8m0-18a10 10 0 0 1 10 10 10 10 0 0 1-10 10C6.47 22 2 17.5 2 12A10 10 0 0 1 12 2m.5 5v5.25l4.5 2.67-.75 1.23L11 13V7z"/></svg> <span class=md-ellipsis> 19 min read </span> </div> </li> </ul> </nav> </li> </ul> </nav> <!-- Table of contents, if integrated --> </div> </div> </div> <!-- Page content --> <article class="md-content__inner md-typeset"> <p>I've spent the last year working with LLMs and writing a good amount of technical content on how to use them effectively, mostly with the help of structured parsing using a framework like Instructor. Most of what I know now is self-taught and this is the guide that I wish I had when starting out.</p> <p>It should take about 10-15 minutes at most to read and I've added some resources along the way that are relevant to you. If you're looking for a higher level, i suggest skimming over the first two sections and then focusing more on the application/data side of things!</p> <p>I hope that after reading this essay, you walk away with an enthusiasm that these models are going to change so much things that we know today. We have models with reasoning abilities and knowledge capacities that dwarf many humans today in tasks such as Mathetical Reasoning, QnA and more.</p> <!-- more --> <p>Please feel free to reach on twitter at @ivanleomk if you're interested to chat more!</p> <h1 id=some-background>Some Background</h1> <h2 id=quick-definitions>Quick Definitions</h2> <p>I'll be using some words here so I'll try to define it</p> <ul> <li>Features: A number that represents something (eg. the number of legs on a car for example)</li> <li>Sequence : A list of numbers or a list of list of numbers. This is what we feed into our model</li> <li>Model : A series of operations that we specify using numbers that we learn through training</li> </ul> <h3 id=deep-learning>Deep Learning</h3> <p>Machine Learning models are hard to train. Traditionally, we've had to rely heavily on curated datasets and architectures for every single task or get experts to curate specific features for each task.</p> <p>In the past 20 years, we got really fast computers and chips that allowed us to train much larger models. This was key because the way we train these models is by</p> <ol> <li>Taking some input (normally a list of numbers)</li> <li>Running it through our model ( getting out a list of numbers )</li> <li>Keeping track of all the operations that got us to the final result using a giant graph we call a computational graph</li> <li>Using an algorithm called back propogation to calculate how we should update the numbers in our model to get better results</li> </ol> <p>Intuitively as our models grow larger ( have more numbers and operations to multiply inside to get a final result ), the amount of data that we need to train them AND the amount of operations to calculate to update the weights increases significantly. The complexity also increases significantly.</p> <p>But, these networks weren't able to deal with/represent inputs that require more complex reasoning since they take a single input and produce a single output.</p> <h3 id=rnns-and-lstms>RNNs and LSTMs</h3> <p>A good example of such tasks are machine translation</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a>Je veux la chemise noire pour la fête de ce soir
</span></code></pre></div> <p>which can be roughly translated to</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a>For the party tonight, I want the black shirt!
</span></code></pre></div> <p>We can't do an exact translation on a word level for many of these sentences. That's when we started using things like RNNs. I'm going to gloss over the exact specifics of how they work but on a high level, it works like this</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1 href=#__codelineno-2-1></a><span class=k>def</span> <span class=nf>rnn</span><span class=p>(</span><span class=nb>input</span><span class=p>):</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2 href=#__codelineno-2-2></a>    <span class=n>hidden_state</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>*</span><span class=nb>len</span><span class=p>(</span><span class=n>hidden_state</span><span class=p>)</span> <span class=c1># Using 0 here cause it&#39;s easy but it could very well be a nested array of arrays!</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3 href=#__codelineno-2-3></a>    <span class=k>for</span> <span class=n>token</span> <span class=ow>in</span> <span class=nb>input</span><span class=p>:</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4 href=#__codelineno-2-4></a>        <span class=n>output</span><span class=p>,</span><span class=n>hidden_state</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=nb>input</span><span class=p>,</span><span class=n>hidden_state</span><span class=p>)</span>
</span><span id=__span-2-5><a id=__codelineno-2-5 name=__codelineno-2-5 href=#__codelineno-2-5></a>    <span class=k>return</span> <span class=n>output</span>
</span></code></pre></div> <p>At each step, we produce two outputs, a hidden state and a output vector. These don't need to be of the same dimensions ( containing the same number of vectors ) and often times, the hidden state will be larger than the output vector.</p> <p><img alt src=images/RNN.png></p> <p>But this was great! Because we now have a way to map a sequence of tokens to another sequence of tokens. This is super flexible and allows us to map from</p> <ul> <li>a sequence of pixels from an image to a sequence of text chunks</li> <li>Sentiment analysis</li> </ul> <p>Eventually some people realised that we could also just use 2 RNNS - one would encode the input into some hidden state and the other would decode it into an output that we want.</p> <div class="language-python highlight"><pre><span></span><code><span id=__span-3-1><a id=__codelineno-3-1 name=__codelineno-3-1 href=#__codelineno-3-1></a><span class=k>def</span> <span class=nf>rnn</span><span class=p>(</span><span class=nb>input</span><span class=p>):</span>
</span><span id=__span-3-2><a id=__codelineno-3-2 name=__codelineno-3-2 href=#__codelineno-3-2></a>    <span class=n>hidden_state</span> <span class=o>=</span> <span class=p>[</span><span class=mi>0</span><span class=p>]</span><span class=o>*</span><span class=nb>len</span><span class=p>(</span><span class=n>hidden_state</span><span class=p>)</span> <span class=c1># Using 0 here cause it&#39;s easy but it could very well be a nested array of arrays!</span>
</span><span id=__span-3-3><a id=__codelineno-3-3 name=__codelineno-3-3 href=#__codelineno-3-3></a>    <span class=k>for</span> <span class=n>token</span> <span class=ow>in</span> <span class=nb>input</span><span class=p>:</span>
</span><span id=__span-3-4><a id=__codelineno-3-4 name=__codelineno-3-4 href=#__codelineno-3-4></a>        <span class=n>output</span><span class=p>,</span><span class=n>hidden_state</span> <span class=o>=</span> <span class=n>model</span><span class=p>(</span><span class=nb>input</span><span class=p>,</span><span class=n>hidden_state</span><span class=p>)</span>
</span><span id=__span-3-5><a id=__codelineno-3-5 name=__codelineno-3-5 href=#__codelineno-3-5></a>    <span class=k>return</span> <span class=n>output</span><span class=p>,</span><span class=n>hidden_state</span>
</span><span id=__span-3-6><a id=__codelineno-3-6 name=__codelineno-3-6 href=#__codelineno-3-6></a>
</span><span id=__span-3-7><a id=__codelineno-3-7 name=__codelineno-3-7 href=#__codelineno-3-7></a><span class=c1># Note these will be two RNNs with separate and different weights</span>
</span><span id=__span-3-8><a id=__codelineno-3-8 name=__codelineno-3-8 href=#__codelineno-3-8></a><span class=n>_</span><span class=p>,</span><span class=n>hs</span> <span class=o>=</span> <span class=n>rnn</span><span class=p>(</span><span class=n>sentence</span><span class=p>)</span>
</span><span id=__span-3-9><a id=__codelineno-3-9 name=__codelineno-3-9 href=#__codelineno-3-9></a><span class=n>o</span><span class=p>,</span><span class=n>_</span> <span class=o>=</span> <span class=n>rnn</span><span class=p>(</span><span class=n>hs</span><span class=p>)</span>
</span></code></pre></div> <p>This worked pretty well for machine translation for some time but RNNs had a major problem.</p> <ol> <li>They process one chunk of input at a time and so it's difficult to deal with long sequences when something near the end of the sequence</li> <li>When we reach the end and finally see whether our prediction matches the result we want, it is dificult to update/make changes to the weight with respect to the first few chunks of inputs.</li> <li>It's difficult to parallelize the processing of multiple chunks - since we need to feed in one token at a time there's no way to skip from the first token to the 100th in a single step. This makes training RNNs on large sequences very difficult since it'll take a long time to do so.</li> </ol> <p>People proposed LSTMs and GRUs ( Which are RNNs that are slightly modified ) to solve this but these didn't achieve the desired result that we wanted.</p> <h3 id=attention>Attention</h3> <p>To do a quick recap, RNNs store their hidden state in a single vector as they process more bits of input. However, this means that some information is lost down the line, especially as we might encounter input tokens that make prior tokens relevant again.</p> <p>Therefore a simple solution emerged, why not we store all of the prior hidden states in memory and allow our model to learn how to combine/weight them?</p> <p><img alt src=images/rnn-attention.png></p> <p>This helped to achieve some good results and greatly improved experimental outcomes. Notably, these hidden states encoded some degree of position because they were incrementally derived from each other, which is something transformer attention needed some tweaks to do.</p> <h1 id=transformers>Transformers</h1> <p>Transformers were proposed as an alternative to these networks with the key idea that instead of processing one input token at a time, we should evaluate the entire sequence at the same time. They incorporated this idea of attention into the architecture of the model and proposed a new variant of attention they called Scaled Dot Product attention. We'll go through a high level overview of what Attention looks like in a transformer in a bit.</p> <p>If there's anything that I think you should take away from this section, it's that Transformers are more than stochastic parrots that are randomly predicting the next token. At each step they have ~50,000 different choices ( depending on the tokenizer). To achieve good performance on this task, they must learn important inforamtion about the world from the data.</p> <p>We can think of them as compressing information into the finite storage that they have. Storage here refers to the number of parameters that they have. Larger models perform better because they can store more information but ultimately how much and how good the data they manage to see during training determines their performance to some degree.</p> <p>They're exciting developments because we can prompt them to do tasks we want. We can fine-tune them on specific datasets to get a specific type of outputs but the fact that we can say stuff like no yapping and have it immediately produce shorter responses is nothing short of pure magic.</p> <h2 id=how-they-work>How they work</h2> <p>Your transformer model never sees text directly, instead it just sees tokens.</p> <p><img alt src=images/tokenization.png></p> <p>We use algorithms such as Byte Pair Encoding and sentence piece and run them on a large corpus of text to choose a selection of tokens that we can use to break down text. The less tokens we can use to represent a specific prompt, the cheaper it is for inference.</p> <p>It might also mean that we get better performance.</p> <p><strong>This also means that languages that are not well represented online typically aren't able to be compressed well and need more tokens</strong></p> <p>On a high level, this is how it works with your transformer</p> <p><img alt src=images/transformer-tokenizer.png></p> <p>On a high level, this is how inference might work with a transformer</p> <p><img alt src=images/transformer-flow.png></p> <p>It has a known mapping of tokens ( think subwords eg. <code>The</code>, <code>France</code>) and each token has a unique id. It then converts the text you pass in to the token ids and outputs a new vector.</p> <p>This vector has the same size as the mapping it has and corresponds to the probability of a token being the next token in the sequence.</p> <p>That's why transformers are often known as auto regressive generative models, because they generate predictions for these tokens AND because future generated output are conditioned on prior output tokens that were chosen to be used.</p> <h2 id=attention-in-transformers>Attention in Transformers</h2> <p>This is a tricky topic and took me some time to understand. I recommend looking at <a href=https://jaykmody.com/blog/gpt-from-scratch/ >GPT in 60 Lines of Numpy</a> and <a href=https://jalammar.github.io/illustrated-gpt2/ >The Illustrated GPT-2</a> which cover this in much greater detail. This is how attention works specifically for decoder-only transformers.</p> <p>Remember how we had the RNN combine different previous hidden states for each individual token? Well, in this case, the magic happens through the following process when we feed our transformer some text.</p> <h3 id=positional-embeddings>Positional Embeddings</h3> <p>First, we break the text down into tokens using a tokenizer.</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-4-1><a id=__codelineno-4-1 name=__codelineno-4-1 href=#__codelineno-4-1></a>An Avocado Shake -&gt; [2127, 7671, 47282, 73423]
</span></code></pre></div> <p>Next, each token gets mapped to a real number vector, This is known as an embedding. Note that this is a vector that is <strong>learnt during training</strong> and is <strong>unique for each token</strong>. It's supposed to encode some semantic meaning about the token itself. I made up some embeddings here</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-5-1><a id=__codelineno-5-1 name=__codelineno-5-1 href=#__codelineno-5-1></a>sample embeddings =&gt; {
</span><span id=__span-5-2><a id=__codelineno-5-2 name=__codelineno-5-2 href=#__codelineno-5-2></a>    2127:  [1,2,3,4],
</span><span id=__span-5-3><a id=__codelineno-5-3 name=__codelineno-5-3 href=#__codelineno-5-3></a>    7671:  [2,2,2,3],
</span><span id=__span-5-4><a id=__codelineno-5-4 name=__codelineno-5-4 href=#__codelineno-5-4></a>    47282: [3,1,2,5],
</span><span id=__span-5-5><a id=__codelineno-5-5 name=__codelineno-5-5 href=#__codelineno-5-5></a>    73423: [8,4,3,2]
</span><span id=__span-5-6><a id=__codelineno-5-6 name=__codelineno-5-6 href=#__codelineno-5-6></a>}
</span><span id=__span-5-7><a id=__codelineno-5-7 name=__codelineno-5-7 href=#__codelineno-5-7></a>
</span><span id=__span-5-8><a id=__codelineno-5-8 name=__codelineno-5-8 href=#__codelineno-5-8></a>[2127, 7671, 47282, 73423]
</span><span id=__span-5-9><a id=__codelineno-5-9 name=__codelineno-5-9 href=#__codelineno-5-9></a>=&gt; [
</span><span id=__span-5-10><a id=__codelineno-5-10 name=__codelineno-5-10 href=#__codelineno-5-10></a>    [1,2,3,4],
</span><span id=__span-5-11><a id=__codelineno-5-11 name=__codelineno-5-11 href=#__codelineno-5-11></a>    [2,2,2,3],
</span><span id=__span-5-12><a id=__codelineno-5-12 name=__codelineno-5-12 href=#__codelineno-5-12></a>    [3,1,2,5],
</span><span id=__span-5-13><a id=__codelineno-5-13 name=__codelineno-5-13 href=#__codelineno-5-13></a>    [8,4,3,2]
</span><span id=__span-5-14><a id=__codelineno-5-14 name=__codelineno-5-14 href=#__codelineno-5-14></a>]
</span></code></pre></div> <p>Notice here that our token would have the same vector irregardless of position. If it was <code>Shake an Avacado</code> vs <code>an Avacado Shake</code>, the Shake here refer to two very different things even if it's the same word.</p> <p>Therefore, we add what's called a positional encoding. This is just a value that we compute for each number in each vector. It's the same regardless of the token, and only depends on the position of the token. More importantly, the model uses this value with a lot of training to be able to understand the position of the token.</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-6-1><a id=__codelineno-6-1 name=__codelineno-6-1 href=#__codelineno-6-1></a>[
</span><span id=__span-6-2><a id=__codelineno-6-2 name=__codelineno-6-2 href=#__codelineno-6-2></a>    [1,2,3,4] + [1,2,3,4]
</span><span id=__span-6-3><a id=__codelineno-6-3 name=__codelineno-6-3 href=#__codelineno-6-3></a>    [2,2,2,3] + [5,6,7,8]
</span><span id=__span-6-4><a id=__codelineno-6-4 name=__codelineno-6-4 href=#__codelineno-6-4></a>    [3,1,2,5] + [9,10,11,12]
</span><span id=__span-6-5><a id=__codelineno-6-5 name=__codelineno-6-5 href=#__codelineno-6-5></a>    [8,4,3,2] + [13,14,15,16]
</span><span id=__span-6-6><a id=__codelineno-6-6 name=__codelineno-6-6 href=#__codelineno-6-6></a>]
</span><span id=__span-6-7><a id=__codelineno-6-7 name=__codelineno-6-7 href=#__codelineno-6-7></a>=&gt;
</span><span id=__span-6-8><a id=__codelineno-6-8 name=__codelineno-6-8 href=#__codelineno-6-8></a>[
</span><span id=__span-6-9><a id=__codelineno-6-9 name=__codelineno-6-9 href=#__codelineno-6-9></a>    [2, 4, 6, 8]
</span><span id=__span-6-10><a id=__codelineno-6-10 name=__codelineno-6-10 href=#__codelineno-6-10></a>    [7, 8, 9, 11]
</span><span id=__span-6-11><a id=__codelineno-6-11 name=__codelineno-6-11 href=#__codelineno-6-11></a>    [12, 11, 13, 17]
</span><span id=__span-6-12><a id=__codelineno-6-12 name=__codelineno-6-12 href=#__codelineno-6-12></a>    [21, 18, 18, 18]
</span><span id=__span-6-13><a id=__codelineno-6-13 name=__codelineno-6-13 href=#__codelineno-6-13></a>]
</span></code></pre></div> <h3 id=attention-mechanism>Attention Mechanism</h3> <p>Before we go into how it works, I want to point out why attention is needed. For a text model, we want our model to be able to generate some understanding of a specific token in relation to the sentence/input.</p> <p>This is important because our model is trained to predict the next token in a sequence. Without a strong understanding of the underlying meaning of each token in the sequence, it's not going to be able to do a good job.</p> <p>Therefore, we use attention as a way to do two things</p> <ol> <li>Create a representation for each individual token</li> <li>Generate a weightage for each token and every token prior to it - we don't want tokens to get information on any of the future tokens after it. That'll be cheating after all</li> </ol> <p>Once we have this, we basically have a new representation for each token that is a weighted sum of all the relevant tokens prior to it. More importantly, there were two big improvements for transformers as compared to RNNs</p> <ol> <li>The entire input sequence could be trained in parallel since we're just doing basic matrix multiplication. Compare this to an RNN which processes each input token incrementally. This meant that we could scale our datasets to much larger sizes, allow our model to learn better representations of data and in turn make better predictions.</li> <li>By splitting up the generated representation using multi-head attention, we can allow our model to learn to process different semantic meaning in each subset of the vector. This means that we could extract much richer representations.</li> </ol> <p>This is on a high level how attention works</p> <p><img alt src=images/mha.png></p> <p>We basically take our input representation and multiply it with three separate matrices and get back three new matrices. These are known as the Query, Key and Value Matrix. It's easiest to think about them as</p> <ul> <li>Query : What each token is looking for</li> <li>Key : What each token can offer to other tokens</li> <li>Value : Some semantic meaning</li> </ul> <p>We multiply our generated Query and Key matrix together and get a weightage for each token at each position. Taking our earlier tokens for example, we might get something like what we see below when we multiply our query and key matrix together.</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-7-1><a id=__codelineno-7-1 name=__codelineno-7-1 href=#__codelineno-7-1></a>[
</span><span id=__span-7-2><a id=__codelineno-7-2 name=__codelineno-7-2 href=#__codelineno-7-2></a>    [1.0, 0.0, 0.0, 0.0],
</span><span id=__span-7-3><a id=__codelineno-7-3 name=__codelineno-7-3 href=#__codelineno-7-3></a>    [0.2, 0.5, 0.0, 0.0],
</span><span id=__span-7-4><a id=__codelineno-7-4 name=__codelineno-7-4 href=#__codelineno-7-4></a>    [0.3, 0.2, 0.5, 0.0],
</span><span id=__span-7-5><a id=__codelineno-7-5 name=__codelineno-7-5 href=#__codelineno-7-5></a>    [0.1, 0.1, 0.3, 0.5]
</span><span id=__span-7-6><a id=__codelineno-7-6 name=__codelineno-7-6 href=#__codelineno-7-6></a>]
</span></code></pre></div> <p>Next, we perform a matrix multiplication of this matrix with the value matrix. This operation aggregates the values for each token across each row, weighted according to the defined percentages in each row. Once we've done so, we feed it through a normal linear network that further enriches this vector of numbers.</p> <p>The way I like to think about it is that the attention operation combines semantic meaning of different tokens at each position while the linear layer helps us to make sense of this new information. We then do this multiple times and the final result helps generate the prediction.</p> <h2 id=training-a-model>Training a Model</h2> <p>So, how are transformers usually trained? Today, the process is complicated with newer models using different methods such as DPO, KTO but we're going to just look at RLHF today. I'll be borrowing some graphics and content from <a href=https://huyenchip.com/2023/05/02/rlhf.html>Chip Huyen's article on RLHF</a>. For a more in-depth look, do refer to that article.</p> <p><img alt src=images/Pipeline.png></p> <p>We'll cover this in three parts</p> <ol> <li>How to train a base model</li> <li>How to get the same base model to understand a prompt template and get a SFT model</li> <li>How to make sure the SFT model outputs good responses</li> </ol> <p>Just to re-iterate. The big plus here now is that instead of fine-tuning on a specific dataset for one-task. These models were trained so that we can get pretty good performance... just by writing a nice paragraph/prompt. This was mindblowing back in the day, even in GPT-2 where they achieved state of the art performance for many benchmarks with a simple written instruction.</p> <p>In short, instead of fine-tuning on the task, these models were fine-tuned to understand and follow instructions which allowed them to take advantage of the information/capabilities that they had learnt during pre-training.</p> <h3 id=pre-training>Pre-Training</h3> <p>Well, how do we make sure that our transformer knows enough so that it outputs a good probability distribution? That's where the pretraining stage comes in.</p> <p>If we have a lot of text (eg.)</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-8-1><a id=__codelineno-8-1 name=__codelineno-8-1 href=#__codelineno-8-1></a>The Capital Of France is Paris. It&#39;s a huge city which is famous for ....
</span></code></pre></div> <p>We can split this into many training examples by taking a slice of text and trying to make the model predict the next completion from its known vocabulary.</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-9-1><a id=__codelineno-9-1 name=__codelineno-9-1 href=#__codelineno-9-1></a>The Capital of France is ---&gt; Paris
</span><span id=__span-9-2><a id=__codelineno-9-2 name=__codelineno-9-2 href=#__codelineno-9-2></a>Capital of France is Paris ----&gt; It&#39;s
</span><span id=__span-9-3><a id=__codelineno-9-3 name=__codelineno-9-3 href=#__codelineno-9-3></a>of France is Paris It&#39;s ----&gt; a
</span></code></pre></div> <p>This is a semi-supervised approach because the training examples used aren't generated by human annotators but instead come naturally from the text itself. The important thing to note here is that our model has to learn an awful lot of information in order to predict the next token accurately,</p> <p>For example</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-10-1><a id=__codelineno-10-1 name=__codelineno-10-1 href=#__codelineno-10-1></a>The President of the United States in 2008 was &lt;prediction&gt;
</span></code></pre></div> <p>This requires the model to be able to</p> <ul> <li>Have a concept of time and what 2008 was</li> <li>Know what a president is</li> <li>Look through its known list of presidents</li> <li>Identify who was the president was at that point of time</li> </ul> <p>Once we train the model with enough data, this becomes what is known as a Base Model. Base models are great but they can't follow instructions.</p> <p>That leads us to the next step - Structured Fine Tuning.</p> <h3 id=sft-structured-fine-tuning>SFT ( Structured Fine Tuning )</h3> <p>Base Models are unable to follow instructions. They might have the knowledge and capability to answer the question but aren't trained to do so. We can see this in an example below from the InstructGPT paper.</p> <p><img alt src=images/sft-result.png></p> <p>We can see that the Base GPT-3 model simply returns a completion. It seems like it might have seen this question inside its training data itself. However, the InstructGPT model ( despite having 100x fewer parameters ) was able to understand the exact instruction and return a valid response.</p> <p>Fundamentally, this just means getting the model itself to learn a specific format of instructions. Here's another example taken from the InstructGPT paper. The <code>[Completion]</code> portion simply indicates where the model was left to generate the text free flow.</p> <p><img alt src=images/InstructGPTPrompt.png></p> <p>Nowdays, these formats have evolved to more complex versions. I've found an example of the <code>OpenAI</code> Chatml format which is frequently found.</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-11-1><a id=__codelineno-11-1 name=__codelineno-11-1 href=#__codelineno-11-1></a>&lt;|im_start|&gt;system
</span><span id=__span-11-2><a id=__codelineno-11-2 name=__codelineno-11-2 href=#__codelineno-11-2></a>You are InternLM2-Chat, a harmless AI assistant&lt;|im_end|&gt;
</span><span id=__span-11-3><a id=__codelineno-11-3 name=__codelineno-11-3 href=#__codelineno-11-3></a>&lt;|im_start|&gt;user
</span><span id=__span-11-4><a id=__codelineno-11-4 name=__codelineno-11-4 href=#__codelineno-11-4></a>Hello&lt;|im_end|&gt;
</span><span id=__span-11-5><a id=__codelineno-11-5 name=__codelineno-11-5 href=#__codelineno-11-5></a>&lt;|im_start|&gt;assistant
</span><span id=__span-11-6><a id=__codelineno-11-6 name=__codelineno-11-6 href=#__codelineno-11-6></a>Hello, I am InternLM2-Chat, how can I assist you?&lt;|im_end|&gt;
</span></code></pre></div> <p>Instead of just merely being an instruction -&gt; response pair, we now have various different roles avaliable, including system roles that might include information that we don't want the user to see.</p> <h3 id=rlhf>RLHF</h3> <p>Now that our model has learnt how to generate valid responses to user prompts, we need to make sure that they're good responses.</p> <p><img alt src=images/rlhf.png></p> <p>The key idea here is that we take a new model ( this could be the same model itself, just with the vocabulary vector modified to produce a single output instead using a linear layer ) and train it to output a single score given a prompt.</p> <p><img alt src=images/rlhf-reward-function.png></p> <p>We're using a log sigmoid function here. So that means that the greater the positive difference between the two generated values, the smaller the loss.</p> <p>There's a key reason why we do so - it's much easier for us to get labellers to compare answers than to write new prompts from scratch. Therefore, we can generate a much larger dataset for the same cost.</p> <p>Since it's easy to get our language model to generate prompts, all we then need to do is make sure that the reward model eventually learns to rank responses similarly to the labellers themselves.</p> <h3 id=current-work>Current Work</h3> <p>InstructGPT was released almost 2 years ago in Mar 2022. What's changed since then?</p> <ul> <li>Model sizes have only grown larger</li> <li>We're still using the transformer architecture ( though some models introduce some different operations or switch the ordering of some operations )</li> <li>People have started experimenting with many different forms of fine-tuning - Some notable exampes are DPO which directly uses the probabilities of the tokens, LLama 2 using two separate reward models for Harmfulness and Helpfulness and even methods like SPIN that take advantage of synthetic data</li> </ul> <h2 id=data-wars>Data Wars</h2> <p>So, now that we know what transformers are and why we should care, let's try and understand why people have been talking about running out of tokens.</p> <p>Here's a useful <a href=https://www.latent.space/p/datasets-101>podcast from latent space about dataset and tokens</a> and an article on <a href=https://denyslinkov.medium.com/why-is-gpt-3-15-77x-more-expensive-for-certain-languages-2b19a4adc4bc>tokenization cost for low resource languages </a></p> <p><img alt src=images/language_models_capacity.png></p> <p>In short, these models scale proportionaly to the amount of tokens they see and the amount of parameters they have. The more parameters they have, the more capacity they have for reasoning and learning. But, that also means that we need significantly more tokens to train these parameters to the right amount. ( See this <a href=https://arxiv.org/abs/2311.05640>paper</a> which describes how to train models with limited data for low resource languages.)</p> <p>The important here is that quality and quantity of tokens is a challenging issue. We want high quality tokens ( Good examples of natural language / sentences ) but also sufficient quantity of tokens so that our model can learn something useful. Important to also note that copyright has become a huge issue in light of this.</p> <p>Let's look at the LLama models for context to see how they've changed over time</p> <ol> <li>Llama 1 : ~ 1 Trillion tokens</li> <li>LLama 2 : 2 Trillion Tokens</li> <li>Llama 3 : 15 Trillion tokens</li> </ol> <p>Interestingly, there've been a lot of new efforts to increase the usage of synthetic data in transformers.</p> <p>Notable efforts include a <a href=https://arxiv.org/html/2401.16380v1>paper by Apple which used a synthetic dataset based off paraphrased content</a>, <a href=https://arxiv.org/abs/2210.11610>using a language models to generate synthethic data to further train itself</a> and more.</p> <p><strong>to be clear, there's a huge amount of information out there, GPT-3 was trained on less than 1% of the internet. What matters is whether we're able to get access to the data and if it makes a difference.</strong></p> <h1 id=using-llms>Using LLMs</h1> <p>Now that we understand what LLMs are and why we care about them, let's look at some common usecases.</p> <p>There are a few different use-cases that will keep popping up</p> <ol> <li>Agents</li> <li>Workflows</li> <li>RAG</li> </ol> <p>Let's walk through them and roughly understand what the challenges are. It's important here to re-iterate that LLMs are difficult to work with because of ther non-deterministic nature.</p> <h2 id=workflows>Workflows</h2> <p>An easy fit for LLMs therefore are simple tasks.</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-12-1><a id=__codelineno-12-1 name=__codelineno-12-1 href=#__codelineno-12-1></a>Prompt: Here&#39;s an email : &lt;email&gt;. Please classify as spam or not spam.
</span><span id=__span-12-2><a id=__codelineno-12-2 name=__codelineno-12-2 href=#__codelineno-12-2></a>Response: Spam
</span></code></pre></div> <p>By prompting the model to perform a specific task, we can use LLMs to easily work with data in different forms. They're very good at understanding text because of how much text they've seen. But, this often means that we might get back nonsense or text in the wrong format.</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-13-1><a id=__codelineno-13-1 name=__codelineno-13-1 href=#__codelineno-13-1></a>Prompt: Here&#39;s an email : &lt;email&gt;. Please classify as spam or not spam.
</span><span id=__span-13-2><a id=__codelineno-13-2 name=__codelineno-13-2 href=#__codelineno-13-2></a>Bad Response: SPAM
</span><span id=__span-13-3><a id=__codelineno-13-3 name=__codelineno-13-3 href=#__codelineno-13-3></a>Bad Response: Thank you for the great question! I think that this is probably a spam email
</span></code></pre></div> <p>It's helpful to slap on some validation on the results. I like using <a href=https://python.useinstructor.com/ >Instructor</a> which provides an easy way to work with structured outputs with your favourite llm provider.</p> <p>If you choose to go down the workflow route, you should in turn be thinking a bit about</p> <ol> <li>How can I capture my outputs? - eg Logfire</li> <li>Can I add in specific validation to my outputs? - eg Pydantic + a second call to another model to validate the output of the first</li> <li>Can I off-load specific portions of a task to other models / start fine-tuning - eg. We generate a lot of data using an LLM. Can we train a BERT model to replace parts of it?</li> </ol> <p>In short, you want to start with expensive models that don't need that much hand holding, get a lot of training data from users then progressively move on to smaller models optimised for one specific section of the task. User data will always be better than any synthetic data you can generate.</p> <p>But, We can do so much more than this! By getting models to output a response in a specific format, we can hook up our models to real-world systems that enable it to perform actions such as send an email, execute terminal commands or even search the web! See <a href=https://gorilla.cs.berkeley.edu/ >Gorilla</a> for an example of a model that is tuned to do so.</p> <h2 id=rag>RAG</h2> <p>RAG is short for retrieval augmented generation. Remember how I mentioned earlier that models are essentially building a world model. This means that they have a fixed state of the world and will not be able to answer questions about specific domains (Eg. your company insurance policies ) or facts that happened that weren't included in their training data (Eg. recent news/events )</p> <p>So, a common use-case is to inject this information into the prompt itself so the model knows what you care about.</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-14-1><a id=__codelineno-14-1 name=__codelineno-14-1 href=#__codelineno-14-1></a>&lt;|im_start|&gt;system
</span><span id=__span-14-2><a id=__codelineno-14-2 name=__codelineno-14-2 href=#__codelineno-14-2></a>You are InternLM2-Chat, a harmless AI assistant&lt;|im_end|&gt;
</span><span id=__span-14-3><a id=__codelineno-14-3 name=__codelineno-14-3 href=#__codelineno-14-3></a>&lt;|im_start|&gt;assistant
</span><span id=__span-14-4><a id=__codelineno-14-4 name=__codelineno-14-4 href=#__codelineno-14-4></a>Here is some relevant information on the user&#39;s query
</span><span id=__span-14-5><a id=__codelineno-14-5 name=__codelineno-14-5 href=#__codelineno-14-5></a>- The latest product offering is our Penguin C2
</span><span id=__span-14-6><a id=__codelineno-14-6 name=__codelineno-14-6 href=#__codelineno-14-6></a>- The C2 is retailing for 200 bucks
</span><span id=__span-14-7><a id=__codelineno-14-7 name=__codelineno-14-7 href=#__codelineno-14-7></a>Make sure to only state what you know&lt;|im_end|&gt;
</span></code></pre></div> <p>By injecting this information into the prompt, the model is able to get access to up to date information, and thus generate better responses. While initially, we can find relevant chunks using embeddings, whole systems will need to be built around these chatbots to reliably/accurately extract and find the right chunks to inject into the prompt on demand. This is a huge and complex decision.</p> <p>See <a href=https://python.useinstructor.com/blog/2023/09/17/rag-is-more-than-just-embedding-search/ >Rag Is More Than Embedding Search</a> for a better overview.</p> <h2 id=agents>Agents</h2> <p>Now, we've covered how to get LLMs to perform specific tasks and get up to date information that wasn't in their training data. So, what happens if we let transformers run wild? Well, that's the idea of an agent.</p> <p>Basically we get the model to keep running until some abstract objective is reached (Eg. Build me a todo list) and that often takes a long time. A cool example of an agent is <a href=https://www.cognition-labs.com/introducing-devin>Devin</a>, a LLM that has been fine-tuned and trained to create/contribute to massive code-bases on its own.</p> <p>Agents became popularized through libraries such as AutoGPT where people used to give a starting prompt and then let the model run wild for many many hours. This was an extension of a prompting framework called <a href=https://www.promptingguide.ai/techniques/react>ReACT</a> which gets the model to generate reasoning before deciding on an action.</p> <p>An example is seen below.</p> <div class="language-text highlight"><pre><span></span><code><span id=__span-15-1><a id=__codelineno-15-1 name=__codelineno-15-1 href=#__codelineno-15-1></a>Question: What is the elevation range for the area that the eastern sector of the
</span><span id=__span-15-2><a id=__codelineno-15-2 name=__codelineno-15-2 href=#__codelineno-15-2></a>Colorado orogeny extends into?
</span><span id=__span-15-3><a id=__codelineno-15-3 name=__codelineno-15-3 href=#__codelineno-15-3></a>
</span><span id=__span-15-4><a id=__codelineno-15-4 name=__codelineno-15-4 href=#__codelineno-15-4></a>Thought 1 : I need to search Colorado orogeny, find the area that the eastern sector
</span><span id=__span-15-5><a id=__codelineno-15-5 name=__codelineno-15-5 href=#__codelineno-15-5></a>of the Colorado orogeny extends into, then find the elevation range of the
</span><span id=__span-15-6><a id=__codelineno-15-6 name=__codelineno-15-6 href=#__codelineno-15-6></a>area.
</span><span id=__span-15-7><a id=__codelineno-15-7 name=__codelineno-15-7 href=#__codelineno-15-7></a>Action 1 : Search[Colorado orogeny]
</span><span id=__span-15-8><a id=__codelineno-15-8 name=__codelineno-15-8 href=#__codelineno-15-8></a>Observation 1 : The Colorado orogeny was an episode of mountain building (an orogeny) in
</span><span id=__span-15-9><a id=__codelineno-15-9 name=__codelineno-15-9 href=#__codelineno-15-9></a>Colorado and surrounding areas.
</span><span id=__span-15-10><a id=__codelineno-15-10 name=__codelineno-15-10 href=#__codelineno-15-10></a>
</span><span id=__span-15-11><a id=__codelineno-15-11 name=__codelineno-15-11 href=#__codelineno-15-11></a>... continue execution
</span></code></pre></div> <p>Langchain also has a similar concept called <code>Chains</code> - allowing users to define specific ReACT agents and chaining these calls together.</p> <h1 id=whats-next>What's next?</h1> <p>Now that we've got a good understanding of roughly some of the latest trends, here are some of the interesting problems that I forsee moving forward.</p> <ol> <li>Evals - Large language models generate free form text. How can we ensure that we're generating input that is valid ? - see <a href="https://www.youtube.com/watch?v=eGVDKegRdgM">Shreya Shankar on Vibe Checks</a> and <a href="https://www.youtube.com/watch?v=yj-wSRJwrrc">Pydantic is all you need by Jason Liu</a></li> <li>Optimizations - as Transformers grow in size and we want to support greater context length, how can we build our models so that they're able to scale well? - See Mixture of Experts , Mixture of Depths, Ring Attention</li> <li>Transformer Alternatives - Is it time to go back to RNNs with alternatives such as RWKV and Mamba and Jamba?</li> </ol> <p>This is a massively evolving field but I hope this was a good first step and introduction.</p> <!-- Add social sharing buttons --> <div class=md-social-share style="margin: 2rem 0; text-align: center"> <span class=md-social-share__label style="display: block; margin-bottom: 1rem; font-weight: 500">Share this post:</span> <div class=md-social-share__buttons style="display: flex; gap: 1rem; justify-content: center"> <a href="https://x.com/intent/tweet?text=Grokking%20LLMs&url=https://ivanleo.com/blog/grokking-llms.html" class=md-social-share__button style="
            display: inline-flex;
            align-items: center;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            background: #000;
            color: #fff;
            text-decoration: none;
            transition: opacity 0.2s;
          " target=_blank rel=noopener> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18.901 1.153h3.68l-8.04 9.19L24 22.846h-7.406l-5.8-7.584-6.638 7.584H.474l8.6-9.83L0 1.154h7.594l5.243 6.932ZM17.61 20.644h2.039L6.486 3.24H4.298Z"/></svg> <span style="margin-left: 0.5rem">Share on X</span> </a> <a href="https://www.linkedin.com/shareArticle?mini=true&url=https://ivanleo.com/blog/grokking-llms.html&title=Grokking%20LLMs&summary=&source=" class=md-social-share__button style="
            display: inline-flex;
            align-items: center;
            padding: 0.5rem 1rem;
            border-radius: 4px;
            background: #0077b5;
            color: #fff;
            text-decoration: none;
            transition: opacity 0.2s;
          " target=_blank rel=noopener> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433a2.06 2.06 0 0 1-2.063-2.065 2.064 2.064 0 1 1 2.063 2.065m1.782 13.019H3.555V9h3.564zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0z"/></svg> <span style="margin-left: 0.5rem">Share on LinkedIn</span> </a> </div> </div> <div class=newsletter-section style="margin: 2rem 0"> <script async data-uid=b184c2f91e src=https://ivan-leo.kit.com/b184c2f91e/index.js></script> </div> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> </main> <footer class=md-footer> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "..", "features": ["content.code.copy", "content.code.annotate", "navigation.tabs", "toc.follow", "navigation.path"], "search": "../assets/javascripts/workers/search.6ce7567c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../assets/javascripts/bundle.83f73b43.min.js></script> <script src=../javascripts/mathjax.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>